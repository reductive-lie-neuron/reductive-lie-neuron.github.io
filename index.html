<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta content="Reductive Lie Neurons (ReLNs): A novel neural network architecture exactly equivariant to general linear symmetries." name="description" />
  <meta content="Equivariant Neural Networks for General Linear Symmetries on Lie Algebras" property="og:title" />
  <meta content="Reductive Lie Neurons (ReLNs): A novel neural network architecture exactly equivariant to general linear symmetries." property="og:description" />
  <meta content="https://reductive-lie-neuron.github.io/static/images/teaser.png" property="og:image" /> <meta content="Equivariant Neural Networks for General Linear Symmetries on Lie Algebras" property="twitter:title" />
  <meta content="Reductive Lie Neurons (ReLNs): A novel neural network architecture exactly equivariant to general linear symmetries." property="twitter:description" />
  <meta content="https://reductive-lie-neuron.github.io/static/images/teaser.png" property="twitter:image" /> <meta property="og:type" content="website" />
  <meta content="summary_large_image" name="twitter:card" />
  <meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />

  <title>Reductive Lie Neurons</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link href="style.css" rel="stylesheet" type="text/css" />
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1/jquery.min.js"></script>
  <script src="script.js" type="text/javascript"></script>

</head>

<body>
  <div class="section">
    <div class="container">
      <div class="title-row">
        <h1 class="title">Equivariant Neural Networks for General Linear Symmetries on Lie Algebras</h1>
        <h1 class="subheader">ICLR 2025 (Under Review)</h1>
      </div>
      <div class="base-row author-row">
        <div class="base-col author-col"><a class="author-text">Chankyo Kim¹*</a></div>
        <div class="base-col author-col"><a class="author-text">Sicheng Zhao*</a></div>
        <div class="base-col author-col"><a class="author-text">Minghan Zhu¹²</a></div>
        <div class="base-col author-col"><a class="author-text">Tzu-Yuan Lin³</a></div>
        <div class="base-col author-col"><a class="author-text">Maani Ghaffari¹</a></div>
      </div>
      <div class="affiliation-row">
        <h1 id="affiliations">¹University of Michigan, ²University of Pennsylvania, ³Massachusetts Institute of Technology</h1>
        <h1 id="affiliations">*Equal contribution</h1>
      </div>

      <div class="link-labels base-row">
        <div class="base-col icon-col">
          <a href="https://arxiv.org/abs/24XX.XXXXX" target="_blank" class="link-block">
            <i class="fa fa-file-pdf-o"></i> Paper (Coming Soon)
          </a>
        </div>
        <div class="base-col icon-col">
          <a href="https://github.com/chankyo123/reductive-lie-neuron" class="link-block">
            <i class="fa fa-github"></i> Code
          </a>
        </div>
      </div>

      <video id="main-video" muted autoplay controls playsinline loop>
        <source id="mp4" src="static/videos/drone_teaser.gif" type="video/mp4"> </video>

      <h2 class="section-header">Abstract</h2>
      <div class="paragraph">
        <p>
          Encoding symmetries is a powerful inductive bias for improving the generalization of deep neural networks. However, most existing equivariant models are limited to simple symmetries like rotations, failing to address the broader class of general linear transformations, GL(n), that appear in many scientific domains. We introduce Reductive Lie Neurons (ReLNs), a novel neural network architecture exactly equivariant to these general linear symmetries... (전체 초록 내용 붙여넣기)
        </p>
      </div>

      <h2 class="section-header">Core Idea: Equivariance by Design</h2>
      <div class="paragraph">
        <p>
          Unlike previous methods, our work introduces a general approach to construct non-degenerate bilinear forms for any n x n matrix Lie algebra, including reductive ones like gl(n). This allows for the principled design of equivariant layers for a much broader class of symmetries.
        </p>
      </div>
      <img src="static/images/applications.png" alt="Applications of ReLN" class="results-img"/>
      <div class="paragraph" style="text-align: center">
          ReLNs are applicable to a wide range of scientific domains governed by diverse Lie group symmetries.
      </div>
      
      <img src="static/images/equivariance_diagram.png" alt="Equivariance Diagram" class="results-img"/>
      <div class="paragraph" style="text-align: center">
        Our network is provably equivariant: a transformation on the input results in the same transformation on the output feature.
      </div>

      <div class="citation add-top-padding">
        <h2 class="section-header"> Citation </h2>
        <p> Our paper is currently under review. If you find our work useful, please cite the ArXiv preprint (link will be available here soon). </p>
        <pre id="codecell0">@misc{kim2024reductive,
      title={Equivariant neural networks for general linear symmetries on Lie algebras}, 
      author={Chankyo Kim and Sicheng Zhao and Minghan Zhu and Tzu-Yuan Lin and Maani Ghaffari},
      year={2024},
      eprint={24XX.XXXXX},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
</pre>
      </div>

    </div>
  </div>
</body>
</html>